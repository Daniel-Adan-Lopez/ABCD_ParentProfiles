# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb  # Import XGBoost
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
#################################
import pandas as pd

# Define file path
file_path = "/data_6_0.feather"

# Load Feather file into DataFrame
df = pd.read_feather(file_path)

# Display basic info about the DataFrame
print(df.info())

# Show first few rows
print(df.head())
###################################
# Select relevant columns
columns = ["participant_id", "screentime_total_combined_minutes", 
           "PSQ_Q1", "PSQ_Q2", "PSQ_Q3", "PSQ_Q4", "PSQ_Q5",
           "PSQ_Q6", "PSQ_Q7", "PSQ_Q8", "PSQ_Q9", "PSQ_Q10",
           "PSQ_Q11", "PSQ_Q12", "PSQ_Q13", "PSQ_Q14", "visit", "ab_g_stc__design_id__fam"]
df = df[columns]
#####################################
# Remove rows where the outcome (screentime_total_combined_minutes) is NA
df = df.dropna(subset=["screentime_total_combined_minutes"])
#####################################
## Show dataset size
print(f"Number of participants: {df['participant_id'].nunique()}")
print(f"Number of observations: {len(df)}")
#Number of participants: 8352
#Number of observations: 24746
####################################
# Split data into training and testing sets **by family ID** to prevent data leakage
unique_families = df["ab_g_stc__design_id__fam"].unique()
####################################
import numpy as np  # Import NumPy

# Split 80% training, 20% testing based on unique families
np.random.seed(123)  # Ensure reproducibility
train_families = np.random.choice(unique_families, size=int(0.8 * len(unique_families)), replace=False)
###################################
## Assign all participants of selected families to the train or test set
train_data = df[df["ab_g_stc__design_id__fam"].isin(train_families)]
test_data = df[~df["ab_g_stc__design_id__fam"].isin(train_families)]
#####################################

# Show train-test split results
print(f"Training set: {len(train_data)} observations from {train_data['participant_id'].nunique()} participants")
print(f"Testing set: {len(test_data)} observations from {test_data['participant_id'].nunique()} participants")
#Training set: 19770 observations from 6678 participants
#Testing set: 4976 observations from 1674 participants
####################################

# Define predictor variables
predictors = ["PSQ_Q1", "PSQ_Q2", "PSQ_Q3", "PSQ_Q4", "PSQ_Q5", 
              "PSQ_Q6", "PSQ_Q7", "PSQ_Q8", "PSQ_Q9", "PSQ_Q10",
              "PSQ_Q11", "PSQ_Q12", "PSQ_Q13", "PSQ_Q14", "visit"]

##################################
# Convert training & test data into DMatrix format
#maintaining the categorical nature of the PSQ items
dtrain = xgb.DMatrix(train_data[predictors], label=train_data["screentime_total_combined_minutes"], enable_categorical=True)
dtest = xgb.DMatrix(test_data[predictors], label=test_data["screentime_total_combined_minutes"], enable_categorical=True)
###################################
#XGBoost only supports categorical features in tree_method="hist" or "approx", so set:
params = {
    "tree_method": "hist",  
    "enable_categorical": True,  
    "objective": "reg:squarederror"
}

##################################
#rechecking the optimal parameters due to now using categorical vs numeric variables for the PSQ items. 
import numpy as np
import pandas as pd
import xgboost as xgb
from xgboost import XGBRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBRegressor

# Define the parameter grid
param_dist = {
    'n_estimators': [100, 300, 500, 700, 900],
    'max_depth': [3, 5, 7, 9],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.5, 0.7, 0.9, 1.0],
    'min_child_weight': [1, 3, 5, 7],
    'gamma': [0, 0.1, 0.2, 0.3]
}

# Initialize the model
xgb_reg = XGBRegressor(objective='reg:squarederror', enable_categorical=True)

# Perform Randomized Search with 5-fold cross-validation
random_search = RandomizedSearchCV(
    xgb_reg, param_distributions=param_dist,
    n_iter=20,  # Number of combinations to try
    cv=5,  # 5-fold cross-validation
    scoring='r2',  # Optimize for R²
    verbose=1,
    n_jobs=-1  # Use all available CPU cores
)

# Fit to the training data
random_search.fit(train_data[predictors], train_data["screentime_total_combined_minutes"])

# Best parameters
best_params = random_search.best_params_
print("Optimal Parameters:", best_params)

# Train the final model with the best parameters
xgb_final = XGBRegressor(**best_params, objective='reg:squarederror', enable_categorical=True)
xgb_final.fit(train_data[predictors], train_data["screentime_total_combined_minutes"])

# Evaluate on test set
test_predictions = xgb_final.predict(test_data[predictors])
train_predictions = xgb_final.predict(train_data[predictors])

# Compute performance metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

r2_test = r2_score(test_data["screentime_total_combined_minutes"], test_predictions)
rmse_test = mean_squared_error(test_data["screentime_total_combined_minutes"], test_predictions) ** 0.5
mae_test = mean_absolute_error(test_data["screentime_total_combined_minutes"], test_predictions)

r2_train = r2_score(train_data["screentime_total_combined_minutes"], train_predictions)
rmse_train = mean_squared_error(train_data["screentime_total_combined_minutes"], train_predictions) ** 0.5
mae_train = mean_absolute_error(train_data["screentime_total_combined_minutes"], train_predictions)

print(f"Test R²: {r2_test:.4f}, RMSE: {rmse_test:.4f}, MAE: {mae_test:.4f}")
print(f"Train R²: {r2_train:.4f}, RMSE: {rmse_train:.4f}, MAE: {mae_train:.4f}")

#Fitting 5 folds for each of 20 candidates, totalling 100 fits
#Optimal Parameters: {'subsample': 0.8, 'n_estimators': 700, 'min_child_weight': 7, 'max_depth': 3, 'learning_rate': 0.01, 'gamma': 0.1, 'colsample_bytree': 0.9}
#Test R²: 0.1204, RMSE: 175.0872, MAE: 125.4282
#Train R²: 0.1282, RMSE: 175.2590, MAE: 126.6682
######################################

#Identify which PSQ items contribute most to the model's predictions
importance_dict = xgb_final.get_booster().get_score(importance_type='gain')
print(importance_dict)\
#{'PSQ_Q1': 308126.53125, 'PSQ_Q2': 452599.65625, 'PSQ_Q3': 584232.5625, 'PSQ_Q4': 895941.5, 'PSQ_Q5': 1826767.875, 'PSQ_Q6': 3901129.75, 'PSQ_Q7': 905457.8125, 
#'PSQ_Q8': 261159.953125, 'PSQ_Q9': 454481.75, 'PSQ_Q10': 207550.75, 'PSQ_Q11': 255098.796875, 'PSQ_Q12': 455612.1875, 'PSQ_Q13': 891689.0, 'PSQ_Q14': 451924.65625, 'visit': 345485.53125}
#############################################
